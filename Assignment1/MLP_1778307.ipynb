{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers, activations\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def derivative_ReLU(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    exp = np.exp(Z - np.max(Z))\n",
    "    return exp / exp.sum(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def init_params(size):\n",
    "    W1 = np.random.rand(10,size) - 0.5\n",
    "    b1 = np.random.rand(10,1) - 0.5\n",
    "    W2 = np.random.rand(10,10) - 0.5\n",
    "    b2 = np.random.rand(10,1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.max()+1,Y.size))\n",
    "    one_hot_Y[Y,np.arange(Y.size)] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "def forward_propagation(X,W1,b1,W2,b2):\n",
    "    Z1 = np.dot(W1, X) + b1 #10, m\n",
    "    A1 = ReLU(Z1) # 10,m\n",
    "    Z2 = np.dot(W2, A1) + b2 #10,m\n",
    "    A2 = softmax(Z2) #10,m\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def backward_propagation(X, Y, A1, A2, W2, Z1, m):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = 2*(A2 - one_hot_Y) #10,m\n",
    "    dW2 = 1/m * (dZ2.dot(A1.T)) # 10 , 10\n",
    "    db2 = 1/m * np.sum(dZ2,1) # 10, 1\n",
    "    dZ1 = W2.T.dot(dZ2)*derivative_ReLU(Z1) # 10, m\n",
    "    dW1 = 1/m * (dZ1.dot(X.T)) #10, 784\n",
    "    db1 = 1/m * np.sum(dZ1,1) # 10, 1\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def update_params(alpha, W1, b1, W2, b2, dW1, db1, dW2, db2):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * np.reshape(db1, (10,1))\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * np.reshape(db2, (10,1))\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y)/Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    size , m = X.shape\n",
    "\n",
    "    W1, b1, W2, b2 = init_params(size)\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, Y, A1, A2, W2, Z1, m)\n",
    "\n",
    "        W1, b1, W2, b2 = update_params(alpha, W1, b1, W2, b2, dW1, db1, dW2, db2)\n",
    "    prediction = get_predictions(A2)\n",
    "    print(f'{get_accuracy(prediction, Y):.3%}')\n",
    "    return W1, b1, W2, b2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.1 Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.113%\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "SCALE_FACTOR = 255\n",
    "WIDTH = X_train.shape[1]\n",
    "HEIGHT = X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\n",
    "X_test = X_test.reshape(X_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR\n",
    "\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.15, 300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.2.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# One-Hot Encoding\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "image_size = X_train.shape[1]\n",
    "input_size = image_size * image_size\n",
    "\n",
    "# resize and normalize\n",
    "x_train = np.reshape(X_train, [-1, input_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(X_test, [-1, input_size])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "batch_size = 64\n",
    "hidden_units = 10\n",
    "dropout = 0.15"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.2.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      " tf.nn.leaky_relu_2 (TFOpLam  (None, 10)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " tf.nn.leaky_relu_3 (TFOpLam  (None, 10)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " tf.nn.softmax_1 (TFOpLambda  (None, 10)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,070\n",
      "Trainable params: 8,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs1 = layers.Input(shape=input_size)\n",
    "hidden1 = layers.Dense(hidden_units, use_bias=True)(inputs1)\n",
    "activation1 = keras.activations.relu(hidden1, alpha=0.3)\n",
    "dropout1 = layers.Dropout(dropout)(activation1)\n",
    "hidden2 = layers.Dense(hidden_units, use_bias=True)(dropout1)\n",
    "activation2 = keras.activations.relu(hidden2, alpha=0.3)\n",
    "dropout2 = layers.Dropout(dropout)(activation2)\n",
    "outputs = layers.Dense(num_labels)(dropout2)\n",
    "activation3 = keras.activations.softmax(outputs)\n",
    "\n",
    "model10 = keras.Model(inputs1, activation3)\n",
    "model10.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9356 - categorical_accuracy: 0.6895 - val_loss: 0.4413 - val_categorical_accuracy: 0.8825\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 1s 992us/step - loss: 0.6149 - categorical_accuracy: 0.8024 - val_loss: 0.3711 - val_categorical_accuracy: 0.8999\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5595 - categorical_accuracy: 0.8228 - val_loss: 0.3501 - val_categorical_accuracy: 0.9044\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5230 - categorical_accuracy: 0.8357 - val_loss: 0.3331 - val_categorical_accuracy: 0.9089\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.5078 - categorical_accuracy: 0.8428 - val_loss: 0.3246 - val_categorical_accuracy: 0.9110\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 1s 991us/step - loss: 0.4898 - categorical_accuracy: 0.8478 - val_loss: 0.3218 - val_categorical_accuracy: 0.9099\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 1s 985us/step - loss: 0.4814 - categorical_accuracy: 0.8504 - val_loss: 0.3108 - val_categorical_accuracy: 0.9103\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 1s 961us/step - loss: 0.4707 - categorical_accuracy: 0.8533 - val_loss: 0.3078 - val_categorical_accuracy: 0.9130\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 1s 968us/step - loss: 0.4680 - categorical_accuracy: 0.8552 - val_loss: 0.3039 - val_categorical_accuracy: 0.9148\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 1s 994us/step - loss: 0.4580 - categorical_accuracy: 0.8561 - val_loss: 0.3011 - val_categorical_accuracy: 0.9161\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 1s 946us/step - loss: 0.4565 - categorical_accuracy: 0.8586 - val_loss: 0.3012 - val_categorical_accuracy: 0.9159\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4598 - categorical_accuracy: 0.8578 - val_loss: 0.3057 - val_categorical_accuracy: 0.9139\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 1s 951us/step - loss: 0.4539 - categorical_accuracy: 0.8604 - val_loss: 0.3091 - val_categorical_accuracy: 0.9145\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4485 - categorical_accuracy: 0.8610 - val_loss: 0.2952 - val_categorical_accuracy: 0.9181\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4428 - categorical_accuracy: 0.8642 - val_loss: 0.2968 - val_categorical_accuracy: 0.9192\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4434 - categorical_accuracy: 0.8631 - val_loss: 0.2972 - val_categorical_accuracy: 0.9162\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 1s 983us/step - loss: 0.4426 - categorical_accuracy: 0.8643 - val_loss: 0.2986 - val_categorical_accuracy: 0.9187\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 1s 973us/step - loss: 0.4374 - categorical_accuracy: 0.8644 - val_loss: 0.2983 - val_categorical_accuracy: 0.9190\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 1s 970us/step - loss: 0.4383 - categorical_accuracy: 0.8656 - val_loss: 0.2901 - val_categorical_accuracy: 0.9209\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 1s 982us/step - loss: 0.4401 - categorical_accuracy: 0.8632 - val_loss: 0.2976 - val_categorical_accuracy: 0.9191\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4397 - categorical_accuracy: 0.8646 - val_loss: 0.2944 - val_categorical_accuracy: 0.9213\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4364 - categorical_accuracy: 0.8640 - val_loss: 0.2915 - val_categorical_accuracy: 0.9203\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 1s 946us/step - loss: 0.4300 - categorical_accuracy: 0.8659 - val_loss: 0.2925 - val_categorical_accuracy: 0.9214\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 1s 950us/step - loss: 0.4310 - categorical_accuracy: 0.8655 - val_loss: 0.2869 - val_categorical_accuracy: 0.9210\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 1s 957us/step - loss: 0.4305 - categorical_accuracy: 0.8664 - val_loss: 0.2976 - val_categorical_accuracy: 0.9194\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 1s 951us/step - loss: 0.4299 - categorical_accuracy: 0.8656 - val_loss: 0.2957 - val_categorical_accuracy: 0.9216\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 1s 946us/step - loss: 0.4287 - categorical_accuracy: 0.8658 - val_loss: 0.2986 - val_categorical_accuracy: 0.9193\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 1s 954us/step - loss: 0.4280 - categorical_accuracy: 0.8647 - val_loss: 0.2904 - val_categorical_accuracy: 0.9204\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 1s 990us/step - loss: 0.4282 - categorical_accuracy: 0.8680 - val_loss: 0.2950 - val_categorical_accuracy: 0.9203\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4260 - categorical_accuracy: 0.8668 - val_loss: 0.2946 - val_categorical_accuracy: 0.9202\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 1s 949us/step - loss: 0.4227 - categorical_accuracy: 0.8678 - val_loss: 0.3004 - val_categorical_accuracy: 0.9206\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 1s 954us/step - loss: 0.4250 - categorical_accuracy: 0.8671 - val_loss: 0.2932 - val_categorical_accuracy: 0.9203\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 1s 927us/step - loss: 0.4231 - categorical_accuracy: 0.8686 - val_loss: 0.2956 - val_categorical_accuracy: 0.9197\n",
      "Epoch 34/50\n",
      "938/938 [==============================] - 1s 941us/step - loss: 0.4221 - categorical_accuracy: 0.8687 - val_loss: 0.2924 - val_categorical_accuracy: 0.9186\n",
      "Epoch 35/50\n",
      "938/938 [==============================] - 1s 935us/step - loss: 0.4234 - categorical_accuracy: 0.8662 - val_loss: 0.2929 - val_categorical_accuracy: 0.9205\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - 1s 933us/step - loss: 0.4188 - categorical_accuracy: 0.8693 - val_loss: 0.2952 - val_categorical_accuracy: 0.9207\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - 1s 942us/step - loss: 0.4146 - categorical_accuracy: 0.8690 - val_loss: 0.2991 - val_categorical_accuracy: 0.9174\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4151 - categorical_accuracy: 0.8690 - val_loss: 0.2932 - val_categorical_accuracy: 0.9188\n",
      "Epoch 39/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4131 - categorical_accuracy: 0.8711 - val_loss: 0.2897 - val_categorical_accuracy: 0.9217\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 1s 973us/step - loss: 0.4127 - categorical_accuracy: 0.8707 - val_loss: 0.2948 - val_categorical_accuracy: 0.9203\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - 1s 993us/step - loss: 0.4133 - categorical_accuracy: 0.8718 - val_loss: 0.2915 - val_categorical_accuracy: 0.9200\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 1s 987us/step - loss: 0.4108 - categorical_accuracy: 0.8724 - val_loss: 0.2905 - val_categorical_accuracy: 0.9186\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4129 - categorical_accuracy: 0.8723 - val_loss: 0.2886 - val_categorical_accuracy: 0.9207\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4125 - categorical_accuracy: 0.8716 - val_loss: 0.2832 - val_categorical_accuracy: 0.9195\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4116 - categorical_accuracy: 0.8722 - val_loss: 0.2843 - val_categorical_accuracy: 0.9215\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.4088 - categorical_accuracy: 0.8730 - val_loss: 0.2928 - val_categorical_accuracy: 0.9185\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 1s 976us/step - loss: 0.4091 - categorical_accuracy: 0.8720 - val_loss: 0.2829 - val_categorical_accuracy: 0.9219\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 1s 959us/step - loss: 0.4032 - categorical_accuracy: 0.8749 - val_loss: 0.2866 - val_categorical_accuracy: 0.9209\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 1s 980us/step - loss: 0.4025 - categorical_accuracy: 0.8745 - val_loss: 0.2845 - val_categorical_accuracy: 0.9221\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 1s 942us/step - loss: 0.4082 - categorical_accuracy: 0.8741 - val_loss: 0.2816 - val_categorical_accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "model10.compile(optimizer=\"adam\", loss=keras.losses.categorical_crossentropy, metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "history10 = model10.fit(x_train, y_train, epochs=50, batch_size=batch_size, validation_data=(x_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "x_train=X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "x_train=x_train / 255.0\n",
    "x_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "x_test=x_test/255.0\n",
    "\n",
    "y_train = tf.one_hot(Y_train.astype(np.int32), depth=10)\n",
    "y_test = tf.one_hot(Y_test.astype(np.int32), depth=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.2.4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape),\n",
    "    keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPool2D(strides=(2,2)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 70s 74ms/step - loss: 0.2342 - categorical_accuracy: 0.9279\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 69s 73ms/step - loss: 0.0835 - categorical_accuracy: 0.9762\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.0644 - categorical_accuracy: 0.9817\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 69s 73ms/step - loss: 0.0515 - categorical_accuracy: 0.9852\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 69s 73ms/step - loss: 0.0469 - categorical_accuracy: 0.9865\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.0402 - categorical_accuracy: 0.9883\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 69s 73ms/step - loss: 0.0370 - categorical_accuracy: 0.9892\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.0326 - categorical_accuracy: 0.9903\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.0299 - categorical_accuracy: 0.9909\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0293 - categorical_accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=keras.losses.categorical_crossentropy, metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final Accuracy Result: 0.9914"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
